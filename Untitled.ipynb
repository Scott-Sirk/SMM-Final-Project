{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3e706d19c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-e3e706d19c6a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwitter_ML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_locn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m#-----------------------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e3e706d19c6a>\u001b[0m in \u001b[0;36mdata_to_vector\u001b[0;34m(self, data, vocab)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                  \u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                  )\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \"\"\"\n\u001b[1;32m   1839\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1127\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1130\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "\n",
    "class Twitter_ML(object):\n",
    "\n",
    "    def __init__(self, file_locn):\n",
    "        self._file_locn = file_locn\n",
    "\n",
    "    def load_data(self):\n",
    "        raw_data = []\n",
    "        for full_path, sub_dirs, files in os.walk(self._file_locn):\n",
    "            for file in files:\n",
    "                answer = full_path.split('\\\\')[-1]\n",
    "                with open(full_path + '\\\\' + file, 'r', encoding = 'utf-8') as f:\n",
    "                    content = f.read()\n",
    "                row = [answer, content, file]\n",
    "                raw_data.append(row)\n",
    "        return raw_data\n",
    "\n",
    "    def process_strings(self, string):\n",
    "        copy = string\n",
    "        copy = copy.lower()\n",
    "        #remove symbols\n",
    "        copy = re.sub('[^a-z ]', ' ', copy)\n",
    "        #string to tokens\n",
    "        tokens = nltk.word_tokenize(copy)\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "        ss = SnowballStemmer('english')\n",
    "        tokens = [ss.stem(word) for word in tokens]\n",
    "        #rebuild string\n",
    "        copy = ' '.join(tokens)\n",
    "\n",
    "        return copy\n",
    "\n",
    "    def data_to_vector(self, data, vocab = None):\n",
    "        text_only = [item[1] for item in data]\n",
    "        answers = [item[0] for item in data]\n",
    "        tf_idf = TfidfVectorizer(ngram_range = (1, 3)\n",
    "                                 , vocabulary = vocab\n",
    "                                 , preprocessor = self.process_strings\n",
    "                                 , tokenizer = nltk.word_tokenize\n",
    "                                 )\n",
    "        X = tf_idf.fit_transform(text_only)\n",
    "        bow = X.toarray()\n",
    "        return bow, answers, tf_idf.vocabulary_\n",
    "\n",
    "    def create_model(self, X, y):\n",
    "        model = SVC()\n",
    "        #SVM has overtaken naive bayes\n",
    "##        model = MultinomialNB()\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    def test_model(self, model, X, y):\n",
    "        print(model.score(X, y))\n",
    "\n",
    "    def use_model(self, model, X):\n",
    "        predictions = model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    file_locn = r'C:\\Users\\Scott\\Desktop\\social_media_mining\\SMM-Final-Project\\train'\n",
    "\n",
    "    obj = Twitter_ML(file_locn)\n",
    "    data = obj.load_data()\n",
    "    bow, answers, vocab = obj.data_to_vector(data)\n",
    "    model = obj.create_model(bow, answers)\n",
    "    #-----------------------------------------------#\n",
    "\n",
    "    test_files = r'C:\\Users\\Scott\\Desktop\\social_media_mining\\SMM-Final-Project\\test'\n",
    "\n",
    "    obj2 = Twitter_ML(test_files)\n",
    "    data2 = obj2.load_data()\n",
    "    bow2, answers2, vocab2 = obj2.data_to_vector(data2, vocab)\n",
    "\n",
    "    obj2.test_model(model, bow2, answers2)\n",
    "\n",
    "    #-----------------------------------------------#\n",
    "    #create predictions and save the data\n",
    "\n",
    "    real_files = r'C:\\Users\\Scott\\Desktop\\social_media_mining\\SMM-Final-Project\\real_data'\n",
    "    obj3 = Twitter_ML(real_files)\n",
    "    data3 = obj3.load_data()\n",
    "    bow3, ans_unknown, vocab3 = obj3.data_to_vector(data3, vocab)\n",
    "\n",
    "    predictions = obj3.use_model(model, bow3)\n",
    "\n",
    "    print('here')\n",
    "\n",
    "    df = pd.DataFrame(columns = ['account'\n",
    "                                 , 'year'\n",
    "                                 , 'month'\n",
    "                                 , 'day'\n",
    "                                 , 'hour'\n",
    "                                 , 'minute'\n",
    "                                 , 'second'\n",
    "                                 , 'collection_period'\n",
    "                                 , 'prediction'])\n",
    "    for i, lst in enumerate(data3):\n",
    "        date, source_file = lst[2].split('__')[:2]\n",
    "        source, ext = source_file.split('.')\n",
    "        year = date[:4]\n",
    "        month = date[4:6]\n",
    "        day = date[6:8]\n",
    "        hour = date[8:10]\n",
    "\n",
    "        minute = date[10:12]\n",
    "        second = date[12:]\n",
    "        predict = predictions[i]\n",
    "        df = df.append({'account':source\n",
    "                        , 'year':year\n",
    "                        , 'month':month\n",
    "                        , 'day':day\n",
    "                        , 'hour':hour\n",
    "                        , 'minute':minute\n",
    "                        , 'second':second\n",
    "                        , 'collection_period':year+month+day+hour\n",
    "                        , 'prediction':predict}\n",
    "                       , ignore_index = True)\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    df.to_csv('final_paper_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
